#APache Spark
=>Apache Spark is a framework for cluster computing 
that was developed as part of a research project at the AMPLab at the
University of California at Berkeley and has been publicly available since 2010

Note1:Distributed Data/Distributed Computing
=>Instead of trying to process a huge dataset or run super computationally-expensive programs on one computer, these tasks can
be divided between multiple computers that communicate with each other to produce an output

Note2: n a distributed computing system, each individual computer is called a node and the collection of all of them is 
called a cluster

*RDD- Resilient Distributed Datasets

Spark APIs: RDD, Dataset and DataFrame
These three APIs can seem very confusing for anyone who’s just getting acquainted with Spark.
https://www.datacamp.com/community/tutorials/apache-spark-python#difference

#Driver program (spark context) =>cluster manager (spark, YARN) --->(Executor, cache, tasks)
Run program upto 100 times faster than map Reduce
DAG(Directed acyclic graph) engine optimizes workflow
amazon, ebay , nasa all uses spark

#components of spark
spark core=> spark streaming, spark SQL, MLlib, GraphX

rdd actions:
collect, count, countByValue, take, top, reduce ..etc

#Processing Engine/Processing Framework :
*Apache Hadoop is an open source software platform that also deals with “Big Data” and distributed computing. Hadoop has a 
processing engine, distinct from Spark, called MapReduce. MapReduce has its own particular way of optimizing tasks to be 
processed on multiple nodes and Spark has a different way. One of Sparks strengths is that it is a processing engine that 
can be used on its own, or used in place of Hadoop MapReduce, taking advantage of the other features of Hadoop


*General-Purpose — One of the main advantages of Spark is how flexible it is, and how many application domains it has. 
It supports Scala, Python, Java, R, and SQL. It has a dedicated SQL module, it is able to process streamed data in real-time

#How is fault tolerance achieved in Apache Spark
=>Spark re computes your RDD before checkpointing it and thus its run 2 times , and when you cache it Spark will simply 
store the cached RDD in memory. And thus in case of failure spark will simply load the latest checkpoint and you will have
a fault tolerant system.

#Lazy Evaluation — Lazy evaluation, or lazy computing, has to do with how code is compiled. When a compiler that is not lazy 
 it sequentially evaluates each expression it comes across. A lazy compiler on the other hand, doesn’t continually evaluate
 expressions, but rather, waits until it is actually told to generate a result and then performs all the evaluation all at 
 once
 
##Spark Terms
RDD:RDD are data structures that are the core building blocks of Spark.RDD is an immutable,partitioned collection of records, 
which means that it can hold values, tuples, or other objects, these records are partitioned so as to be processed on a 
distributed system, and that once an RDD has been made, it is impossible to alter it

#DataFrames. Not to be confused with Pandas DataFrames, as they are distinct, Spark DataFrame have all of the features of 
RDDs but also have a schema. This will make them our data structure of choice for getting started with PySpark

#Spark DataSets:These are similar to DataFrames but are strongly-typed, meaning that the type is specified upon the creation
of the DataSet and is not inferred from the type of records stored in it. This means DataSets are not used in PySpark 
because Python is a dynamically-typed language

For the rest of these explanations I’ll be referring to RDDs but know that what is true for an RDD is also true for a 
DataFrame, DataFrames are just organized into a columnar structure.

##Transformations — Transformations are one of the things you can do to an RDD in Spark. They are lazy operations that 
create one or more new RDDs. It’s important to note that Transformations create new RDDs because, remember, RDDs are 
immutable so they can’t be altered in any way once they’ve been created. So, in essence, Transformations take an RDD as an 
input and perform some function on them based on what Transformation is being called, and outputs one or more RDDs

##Actions — An Action is any RDD operation that does not produce an RDD as an output. Some examples of common Actions are 
doing a count of the data, or finding the max or min, or returning the first element of an RDD

##Lineage Graph — Most of what a lineage graph is was described in the Transformations and Actions sections, but to summarize
a lineage graph outlines what is called a “logical execution plan”. What that means is that the compiler begins with the 
earliest RDDs that aren’t dependent on any other RDDs, and follows a logical chain of Transformations until it ends with the
RDD that an Action is called on

## Spark Application and job:
A Spark application is a user built program that consists of a driver and that driver’s associated executors. A Spark job is
task or set of tasks to be executed with executor processes, as directed by the driver. A job is triggered by the calling of
an RDD Action


##Coding in PySpark:
=>Because we want to be working with columnar data, we’ll be using DataFrames which are a part of Spark SQL
NOTE: To avoid possible confusion, despite the fact that we will be working with Spark SQL, none of this will be SQL code.
You can write SQL queries when working with Spark DataFrames but you don’t have to

#Basic template in pyspark
from pyspark import SparkConf, SparkContext
import collections

conf = SparkConf().setMaster("local").setAppName("RatingsHistogram")
sc = SparkContext(conf=conf)

lines = sc.textFile("path.../ml-100k/u.data")
ratings = lines.map(lambda x: x.split()[2])
result = ratings.countByValue()

sortedResults = collections.OrderedDict(sorted(result.items()))
for key, value in sortedResults.items():
    print("%s %i" % (key, value))

*SparkContext= It is a fundamental startingpoint that the spark framework gives you to create RDD's form
*SparkConf:  SparkConf allow u to configure SparkContext and tells things like do u want to run in a single computer (node)
            or run in a cluster ( lots of connected computer(nodes))
 
 * collections: to sort the final results. Collections in Python are containers that are used to store collections of data,
                for example, list, dict, set, tuple etc
                
*SparkConf().setMaster("local").setAppName("RatingsHistogram")
Here we are setting local computer(first node) as master , for distributed sys we will use distributed instead of local
The appName parameter is a name for your application to show on the cluster UI, here we have only one node(local), no clustr

*sc = SparkContext(conf=conf)
using SparkConfig we create SparkContext object and assign that to something called sc
 

* lines = sc.textFile("<path of file>")
=> A very commomn way of creating an RDD is through the sc.textFile method
so first value of  RDDD (here called as line) is going to be this entire  line of text as a row

* ratings = lines.map(lambda x: x.split()[2])
it will take 3rd column which is ratings

*result = ratings.countByValue()
Now those ratings value (1,2,3,4,5) is counted for each and totalled and saved in result as a list of rating(str) with value(int)
eg 3,3,5,5,5,2,1,1 so here value of ratings or no of total occurance of each rating are  (3,2), (5,2),(2,1),(1,2),(4,0)

*sortedResults = collections.OrderedDict(sorted(result.items()))
now those list value become key value pair using dict & if those ratings ar unordered than it will be ordered(sorted) as 
(1,2),(2,1),(3,2),(4,0),(5,3)

*for key, value in sortedResults.items():
    print("%s %i" % (key, value)).               #note s-->string, i-->intezer

str int 
1    2
2    1
3    2
4    0
5    2







*Is reduceByKey() transformation or action ?
reduceByKey is a transformation, the rule is any operations that creates a new RDD out of an existing RDD is a 
transformation.

#Key/Value
I'm trying to understand the key value and how does Spark know which is the key and  which is the value
I've changed the order of return(age, numFriends)  to return(numFriends, age) and I've realized that by doing that Spark 
sets numFriends as the key, so I assume that the first column is always the key. Am I right?
First entry in the tuple is considered as key for Spark RDD.

#reduceByKey(): combine value with same key using some function 
eg rdd.reduceBykey(lambda x,y: x+y)
groupByKey(): Group value with same key
sortByKey(): sort rdd by key values
keys(), values(): create values of of just keys , or just values

#Dictionaries in Python vs RDD dictionary
In Python, when we create dictionaries we cannot have two values for the same key.

For example: {'a':2,'b':3,'a':5} is not allowed in python but {'a':2,'b':3} is allowed.
But rdd = lines.map(parseLine) this line will create dictionaries with  age as key and number of friends as value. That 
means it is allowing {'a':2,'b':3,'a':5} this format where 'a' and 'b' are ages in our case.
Yes Rahul, RDD map will allow duplicate keys, we can always combine same key values in latter reduce phases depending on 
the use case

#Based on the csv file i downloaded, shouldn't the filter be (lambda x: "TMIN" in x[2])?
The filter is run on the RDD created by parseLine function which produces "stationID, entryType, temperature" all other col
including date column is eliminated
so x[1] here equates to entryType which is what we need to filter.

*Note :%.2f" % total
% is a symbol for variable and is not a modulo!
. and the number fallowing modifies how many digit decimal points you want to print
%f string formatting option treats the value as a decimal, and prints it to six decimal places
the second % outside of the " " sets the variable total to the variable %.2f inside the " "
"%.4f" would print 54.6293
"%.2f" prints 54.63 rounded


#when to use collect()?
Hello Ludwig,
countByValue will return a python list (and not RDD) that is why it is failing as there is no collect needed in that case.
collect() can be run on RDD's only to bring all the parallelized data to the driver script as a python list.
countByValue is doing the same as collect internally.

#Diff between map and flatmap in spark (for understanding)
map(): map transform each element of an RDD into different new element but no of line of data of i/o will be same
flatmap(): it will transform each element of an RDD into new row , so no of words per line in input/output not same

*Text normalization is the process of transforming text into a single canonical form that it might not have had before

###Running spark on a cluster
* YARN is a hadoop cluster manager
and we are running spark on top of it. By the way spark has also on its own cluster manager and can run on aws EC2 instance
for computing

# Some popular operations on RDD
join(), lefOuterJoin(), rightOuterJoin(), groupByKey(), reducerByKey(), CombineByKey()
cogroup(), lookup()
groupwith()

For larger operations which will be required to run cluster on lot of nodes on internet
.partitionBy()

Note: on aws
can use -master yarn to run on a YARN cluster
EMR sets this up by default

#Troubleshooting of cloud cluster
 1)Hadoop cannot  recover from poorly structuring your code
 2) having a wrong number of partitions (1oo is ok for 1 million dataset)
 3) Executor having not enough memeory (it should be 1 GB at least) and asking much more from them to process
 
 Above challeneg should be fix before running a production on cloud cluster
 
 #When to use RDDs?
Consider these scenarios or common use cases for using RDDs when:
you want low-level transformation and actions and control on your dataset;
your data is unstructured, such as media streams or streams of text;
you want to manipulate your data with functional programming constructs than domain specific expressions;
you don’t care about imposing a schema, such as columnar format, while processing or accessing data attributes by name or 
column; and
you can forgo some optimization and performance benefits available with DataFrames and Datasets for structured and 
semi-structured data.

#spark Streaming
Apache Spark Streaming is a scalable fault-tolerant streaming processing system that natively supports both batch and
streaming workloads.





